Controllable Shadow Generation with
Single-Step Diffusion Models from Synthetic Data
Onur Tasar
Jasper Research
onur.tasar@jasper.aiCl´ement Chadebec
Jasper Research
clement.chadebec@jasper.aiBenjamin Aubin
Jasper Research
benjamin.aubin@jasper.ai
(a) Direction control by moving the light source horizontally.
 (b) Direction control by moving the light source vertically.
(c) Softness control.
 (d) Intensity control.
Figure 1. Our single-step model enables the generation of realistic shadows with precise control over their direction, softness, and intensity.
Abstract
Realistic shadow generation is a critical component for
high-quality image compositing and visual effects, yet ex-
isting methods suffer from certain limitations: Physics-
based approaches require a 3D scene geometry, which is
often unavailable, while learning-based techniques strug-
gle with control and visual artifacts. We introduce a novel
method for fast, controllable, and background-free shadow
generation for 2D object images. We create a large syn-
thetic dataset using a 3D rendering engine to train a diffu-
sion model for controllable shadow generation, generating
shadow maps for diverse light source parameters. Through
extensive ablation studies, we find that rectified flow objec-
tive achieves high-quality results with just a single sampling
step enabling real-time applications. Furthermore, our ex-
periments demonstrate that the model generalizes well to
real-world images. To facilitate further research in evalu-
ating quality and controllability in shadow generation, we
release a new public benchmark containing a diverse set of
object images and shadow maps in various settings. The
project page is available at this link.1. Introduction
Generating shadows for object images is crucial for visually
appealing content creation with a wide range of real-world
applications such as product photography, packshots, and
online marketing [49]. A common approach for addressing
this task entails constructing a 3D scene geometry based
on an object image and configuring a light source to render
the desired shadows by employing advanced rendering al-
gorithms such as ray tracing [43]. The cumbersome process
of first creating a 3D scene and then executing a rendering
algorithm is time-consuming, making this approach imprac-
tical for real-world applications. An alternative approach is
to operate directly on images to predict shadows from the
provided input. We focus on this strategy as it bypasses the
lengthy 3D construction and rendering steps.
The main challenge in training a model that generates
shadows from images is collecting a dataset suitable for this
task. Requesting professional annotators to manually cre-
ate shadows for object images is excessively labor-intensive
and costly. It is also challenging for them to create geo-
metrically and physically correct shadows, as these shad-
ows are not generated by an actual light source. An evenarXiv:2412.11972v1  [cs.CV]  16 Dec 2024

more complex challenge is to give a model control over
specific attributes, such as direction, softness, and inten-
sity of the generated shadows, as illustrated in Fig. 1, to
accommodate a wide range of applications. There has been
significant progress in realistic image generation, especially
using diffusion models with various conditionings such as
text [19, 34, 35], depth maps [20, 33, 56], segmentation
masks [20, 56] etc. However, to the best of our knowl-
edge, there is no prior work on conditioning diffusion mod-
els for controllable shadow generation based on the afore-
mentioned specified light source attributes.
Over time, rendering engines have achieved a great level
of realism that makes their generated images nearly in-
distinguishable from actual photos. In addition to render-
ing high-quality images, they are also able to create cor-
responding accurate pixel-wise annotations [4] such as se-
mantic segmentation maps, depth maps, surface normals,
and shadow maps with various light source settings, thereby
automating the excessively time-consuming and costly data
annotation process without the need for external annotators.
Although rendering engines are excellent tools for generat-
ing synthetic annotated data, their use and generalization on
real-world images have largely been under-explored in the
research community, especially for shadow generation.
Given these challenges and limitations, we present a new
fast and controllable shadow generation pipeline that is in-
dependent of the background image. To this end, we first
generate a large synthetic dataset that includes high-quality
images and shadows with various directions and softness by
positioning a light source and moving across the surface of
a sphere. We then train a single-step diffusion model con-
ditioned on the spherical coordinates and other parameters
of the light source, predicting controllable shadows as gray-
scale images. We finally blend the object image and the
predicted shadows into given target background images.
The main contributions of the paper are as follows:
• Presenting a new shadow generation pipeline for object
images that is robust to varying backgrounds.
• Creating a large synthetic dataset from a diverse collec-
tion of 3D meshes and showing that our model trained on
fully synthetic data generalizes effectively to real images.
• Training a one-step diffusion model by proposing a sim-
ple yet effective conditioning mechanism to inject light
source parameters, such as the spherical coordinates and
size, allowing the control of shadow properties.
• Performing extensive ablation studies on several diffusion
prediction types, number of sampling steps, number of
training iterations, and various conditioning mechanisms.
• Publicly releasing three curated public test sets of a di-
verse distribution of objects to evaluate the performance
of future works in the community for shadow shape con-
sistency, shadow direction and softness control.2. Related Works
Diffusion models [10, 44, 45], have been demonstrated to
represent the current state-of-the-art in generative modeling
for image synthesis [5, 19, 34, 35]. Using diffusion mod-
els, various dense prediction tasks like surface normal pre-
diction [55, 56], depth map estimation [8, 14, 39, 56], and
image matting [12, 50] have also been explored. One major
limitation of diffusion models for real-world applications
is their slow inference time due to the iterative de-noising
sampling process [10]. There have been attempts to achieve
one or a few steps of sampling by using latent consistency
models [29], distillation [1, 9, 27, 30, 38, 53] and flow
matching [21]. The authors of lotus [8] proposed a single-
step dense prediction approach based on a pre-trained dif-
fusion model. However, fast and controllable shadow gen-
eration with diffusion models has remained unexplored.
Specifically for controllable shadow generation, non-
diffusion-based methods have been proposed. For instance,
SSN [40] presents a framework with two U-Nets [36]. The
former predicts an ambient occlusion map and the latter out-
puts the final shadows. The main limitation of this work is
that the performance of the first U-Net strongly depends on
the view angle of the object. To mitigate this issue, this
research has then been extended by replacing the first U-
Net by pixel-high map estimation [41]. Another extension
of this work is PixHt-Lab [42], which also predicts reflec-
tions. However, this approach requires several intermediate
steps—such as predicting surface normals and a depth map,
followed by rendering—which makes the process lengthy.
There exists a line of research focusing on generating
shadows for an object by analyzing the background scene,
and the direction and intensity of the shadows of other ob-
jects within the image. This approach is specifically inves-
tigated using generative adversarial networks [6] in Des-
oba [11] and ShadowGAN [57], and using ControlNets [56]
in Desobav2 [23]. ObjectDrop [51] blends a foreground ob-
ject into a background image by generating shadows and re-
flections consistent with the background scene using diffu-
sion models. In addition to generating reflections and shad-
ows, ObjectStitch [46] also changes the geometry and color
of the foreground object by merging it with a background
image. However, these approaches lack flexibility in con-
trolling shadow direction, softness, and intensity.
Realistic and controllable shadow generation can also
be achieved by performing an image-to-3D mesh genera-
tion followed by rendering shadows using the mesh [24].
Although recent research has achieved remarkable perfor-
mance on image-to-3D model tasks using neural radiance
fields [3, 31, 32, 52], diffusion models [22, 25], and Gaus-
sian Splatting [15], they are not well suited towards the
shadow generation for single object image task, since they
either require multiple views of the object for 3D mesh gen-
eration or have long processing times.

Figure 2. Example renders with unprocessed (first two) and pro-
cessed (last two) meshes. The red line represents the ground.
(a) Scenes with varying area light sizes. The pyramid represents the cam-
era. The area light is indicated by the square with a circle in the middle.
(b) Renders with the area lights having sizes 1, 2, and 3 respectively.
Figure 3. The effect of the area light size on the shadow’s softness.
The area light sizes are 1, 2, and 3 respectively from left to right.
3. Method
Creating a large, high-quality synthetic dataset and training
a model on it for fast inference and precise control over spe-
cific shadow properties are two crucial components of our
pipeline. We start by providing a comprehensive overview
of our synthetic dataset creation in Sec. 3.1, and we present
our shadow generation training pipeline in Sec. 3.2.
3.1. Synthetic Dataset
Manually annotating images to build a dataset for the
shadow generation task is inefficient, as it requires signif-
icant labor and poses challenges in generating geometri-
cally accurate shadows without incorporating physics. We
overcome these challenges by creating a synthetic dataset,
which is therefore a crucial component in our pipeline.
We collected a large set of high-quality 3D meshes cre-
ated by professional artists, licensed for free of use, cover-
ing a wide range of object categories (see SM. 6 for more
details). However, the 3D meshes created by the artists are
usually placed at random locations in 3D space. As a re-
sult, when a 3D mesh is positioned above the ground, the
shadow cast by the light source often does not connect with
the object. This creates the illusion that the object is floating
in the air, as illustrated in Fig. 2. Training a model on such
yz
xArea light
θ
ϕs
r
3D model
CameraFigure 4. Spherical coordinate system. θ,ϕ, and rrepresent the
polar angle, azimuthal angle, and the radius. scorresponds to the
size of the area light. We place the camera at negative y-axis.
(a) Image
 (b) Mask
 (c) Shadow Map
Figure 5. An example image from our dataset and its annotations.
images would likely lead to the prediction of similar erro-
neous shadows positioned below the object. To overcome
this limitation, we place a sufficiently large temporary hori-
zontal plane on the ground, apply a rigid body physics [47]
to each mesh, drop the 3D mesh from a certain height, and
finally remove the plane. This pre-processing step is nec-
essary to place the meshes on the ground to connect the
shadow with the object. Fig. 2 shows example meshes be-
fore and after this processing step.
We use Blender13D engine to create our synthetic
dataset. To render shadows for a given 3D mesh, we em-
ploy an area light , a light source that emits light from a de-
fined two-dimensional shape, such as a rectangle, square, or
disc. We prefer to use a square area light, because it allows
us to easily adjust the softness of the shadows by simply
changing the size of the square area light. A smaller area
light produces sharper shadows, while a larger one results
in softer, more diffused shadows. Fig. 3 illustrates how the
shadow’s softness evolves when changing the light size s.
We propose to use the spherical coordinate system to po-
sition the light, where its location is determined by the polar
angle θ, the azimuthal angle ϕ, and the radius of the sphere
r. We place the 3D mesh at the center and the camera at
the negative y-axis. Given a set of light source parameters
1https://www.blender.org

S(θ, ϕ, s ), we render an image of the object with its shadow
(Fig. 5a), a binary object mask (Fig. 5b), and a gray-scale
shadow map (Fig. 5c) during each rendering iteration. Fig. 4
illustrates all light source parameters and the positioning of
the camera and the 3D object within a spherical coordinate
system. The intensity of the rendered shadow map can be
easily adjusted by multiplying it with a scalar I.
3.2. The Shadow Generation Pipeline
Our second main contribution focuses on training a model
using our synthetic dataset, outlined in Sec. 3.1, to predict
shadows. We propose developing a diffusion model con-
ditioned on both the object image and the light parameters
S(θ, ϕ, s )to predict a controllable shadow map in a single
step. In Sec. 3.2.1, we provide an overview of diffusion
models and the various prediction types for training. Addi-
tionally, in Sec. 3.2.2, we revisit rectified flow [26], which
has proven effective in generating shadows in just one step.
3.2.1. Background on Diffusion Models
Diffusion models [10, 44, 45] are probabilistic generative
models that transform a simple noise distribution into a
complex data distribution such as natural images, through
a series of forward and reverse processes [2, 16, 54]. In the
forward process, random noise is gradually added to the in-
put data over a sequence of timesteps, transforming it into
simple Gaussian noise. The reverse process aims to reverse
this gradual corruption of the data by learning to denoise
the noisy data. This allows for the generation of realistic
samples from pure noise during the inference stage [10].
To reduce the computational complexity involved in
training a diffusion model on high dimensional data, such
as images, a common practice is to employ an auto-
encoder [17] with an encoder E(·)and a decoder D(·), to
respectively map an input image into a smaller latent space
and decode it back to the pixel space, i.e., E(x) =zxand
D(zx)≈x, where x∈ X is a set of input images deriving
from an unknown distribution [35].
The forward process is controlled by two differentiable
functions α(t),σ(t)for any t∈[0,1]as follows:
zx
t=α(t)·zx
0+σ(t)·εwith ε∼ N (0,I),(1)
where zx
0represents the embeddings computed by Efrom
the input image x0. Astgoes to 1, the noisy sample even-
tually resembles pure noise. In practice, a diffusion model
consists in learning a function fψparametrized by a set of
parameters ψ, conditioned on the timestep tand taking as
input the noisy sample zx
t. Different choices of parametriza-
tion exist for fψleading to the following objectives:
•ε-prediction [10]: predicting the amount of noise added
tozx
0in Eq. (1), where the loss is
L=Ez,t,ε
∥ε−fψ(zx
t)∥2
, (2)•sample-prediction : predicting the clean latents as
L=Ez,t,ε
∥zx
0−fψ(zx
t)∥2
, (3)
•v-prediction [37]: which can be regarded as a combina-
tion of the two prediction types above
L=Ez,t,ε
∥(α(t)·ε−σ(t)·zx
0)−fψ(zx
t)∥2
.(4)
3.2.2. Rectified Flows
Letp0be an unknown target data distribution and p1a sim-
ple distribution easy to sample from, Flow Matching [21]
is a type of generative model that aims to estimate a time-
dependent vector field vtwhere the vector field defines the
following Ordinary Differential Equation (ODE).
dxt=vt(xt, t)dt x 0∼p0, x1∼p1, t∈[0,1].(5)
The main idea behind flow matching is to estimate the vec-
tor field allowing to interpolate between p0andp1with a
parametrized function fψ. In the specific case of Rectified
Flows [26], the vector field is trained to be constant and to
follow the direction x1−x0using the following objective
min
ψEx0,x1,t
∥(x1−x0)−fψ(xt, t)∥2
, (6)
where xt=x1·t+ (1−t)·x0,(x0, x1)∼p0×p1andt
is sampled from a given timestep distribution π.
3.2.3. Shadow Generation
Our controllable shadow generation pipeline employs
SDXL architecture [34] as its backbone, but we remove all
the cross-attention blocks originally used for text condition-
ing. Instead of using its original objective, we train it using
a Rectified Flow objective Eq. (6). Since the V AE of SDXL
has been trained to compress RGB images, we transform the
target gray-scale shadow map into an RGB image by repli-
cating it twice and concatenating them with the original.
At inference, we use only the first channel as the predicted
shadow. We reverse the predicted shadow map and finally
blend it with the object image and the target background to
produce the final output. Fig. 6 depicts the full pipeline.
We condition the diffusion model fψon the object im-
ageoand its mask mto enforce the model to encode the
geometry of the object. To achieve this, we map the object
image o∈R3×H×Wand its corresponding binary mask
m∈R1×H×Wto the latent space by encoding with the
frozen encoder E,zo=E(o)∈Rc×h×w,zm=F(m)∈
R1×h×wwithW/w =H/h = 8, where c= 4, and F
denotes the resizing operator. As the mask and the fore-
ground are spatially aligned with the final output, we di-
rectly concatenate these conditionings to the noisy latent:
zx
t←[zx
t, zm, zo]∈R(2c+1)×h×w. Since the input latents
to the denoising network have more channels, 2c+ 1, than

Figure 6. Our controllable shadow generation pipeline. We first remove the background of the input image, providing us with a binary
mask. The V AE embeddings computed from the background-free input, and the resized mask are concatenated with the noise in the latent
space. The denoiser fψis also conditioned on the light parameters S(θ, ϕ, s )through timestep embeddings to predict controllable shadows.
We reverse the predicted shadow map, blend it with the object image and the target background to produce the final output.
Data Param. Interval #Imgs.
Training θ [0◦,1◦, ...,45◦]
Data ϕ [0◦,1◦, ...,360◦] 257,612
9,872 models s [2,3, ...,8]Track 1Softness θ 30◦
Control ϕ 0◦150
50 3D models s [2,4,8]Track 2Horz. Direc. θ 35◦
Control ϕ [0◦,20◦, ...,360◦] 270
15 3D models s 2Track 3Vert. Direc . θ [5◦,10◦, ...,45◦]
Control ϕ 0◦135
15 3D models s 2
Table 1. Light source parameters used to create the synthetic data.
the original SDXL denoiser, c, we introduce new parame-
ters in the first convolution block, initialized to zero. All
other parameters are initialized using the SDXL weights.
To condition fψon the light parameters we propose to
encode the scalar light source parameters S(θ, ϕ, s )with a
sinusoidal embedding [10], already used in SDXL [34] to
encode the timesteps t, as follows:
e(t) =h
cos 
ωd
i·t	d/2−1
i=0,
sin 
ωd
i·t	d/2−1
i=0i
,(7)
with the frequency ωd
i= 2−i·(i−1)
d/2·(d/2−1)log(10000),∀i∈
[0, d/2−1], where d= 256 is the projection embedding
dimension. The denoiser fψis therefore conditioned on the
vector [e(θ), e(ϕ), e(s)]∈R768, which is added to the usual
timestep embedding. We use the above conditioning mech-
anism that allows to directly inject scalars into the denoiser,
rather than requiring the creation of an external representa-
tion for the light parameters as proposed in SSN [40].4. Experiments
We start by outlining in Sec. 4.1 the process of creating
our synthetic dataset, a crucial component of our pipeline.
Next, we include in Sec. 4.2-4.3 detailed ablation studies
and present in Sec. 4.4 qualitative results on real images.
4.1. Dataset
To enable the model to generalize across a broad range of
objects, it is crucial to gather a large dataset encompass-
ing diverse high-quality 3D meshes. We collected 9,922 3D
meshes manually designed by artists representing a wide
variety of real-world objects (see more details in SM. 6).
We use 50 models to create a test set and consider the
remaining 9,872 models for the trainings. We rendered
257,612 training images with (W, H )=(1024 ,1024) reso-
lution in Blender using cycles rendering engine [13], a ray-
trace-based production render engine [43]. Although it is
substantially slower, we prefer using cycles over engines
likeeevee [7], since it delivers the highest quality.
To diversify the number of views for each object when
creating the training set, in each rendering iteration, we in-
stantiate a randomly selected 3D mesh at the center of the
spherical coordinate system shown in Fig. 4 and rotate it
around z-axis by a random degree between 0◦and360◦.
We also position the camera at the negative y-axis and ran-
domly move it along the y-axis to have images with varying
object scales. We resize the 3D mesh proportionally by ad-
justing its largest dimension to a fixed value to maintain an
appropriate scale, avoiding excessively large or small sizes.
We set the sphere radius rto 8. We randomize the light pa-
rameters S(θ, ϕ, s )(see Sec. 3.1) by selecting a value from
the intervals indicated in Table 1. We render the shadow
maps with a fixed intensity.
With no existing dataset available to evaluate our

(a) Track 1: θ= 30◦,ϕ= 0◦, ands= [2,4,8]
from left to right.
(b) Track 2: s= 2,θ= 35◦, and
ϕ= [40◦,100◦,220◦]left to right.
(c) Track 3: s= 2,ϕ= 0◦, and
θ= [5◦,20◦,35◦]from left to right.
Figure 7. Example renders from each test track. Track 1: Softness control. Tracks 2-3: Horizontal and vertical shadow direction control.
012 4 8 2000.20.40.60.81
Number of sampling stepsIoUε-prediction rectified flow sample prediction v-prediction
010k 30k 50k 70k 90k 110k 130k 150k00.20.40.60.81
Number of training iterationsIoU
Figure 8. IoU vs number of sampling (first plot) and training iter-
ation plots (second plot). In the first plot, the number of training
iterations for each model is fixed to 150k. In the second plot, the
number of sampling step is set to 20. Half transparent curve thick-
ness represents the standard deviation.
pipeline’s performance, we decide to create a new bench-
mark specifically for this task and make it publicly acces-
sible. Our new test set includes three tracks, each carefully
designed to assess the model’s performance in controlling
shadow softness, as well as horizontal and vertical direc-
tion. We create the samples for each track as:
•Track 1 : Softness control. We fix θandϕand use 3
different values for s.
•Track 2 : Horizontal direction control. We fix θandsand
use 18 different values for ϕ.
•Track 3 : Vertical direction control. We fix ϕandsand
employ 9 distinct values for θ.
Table 1 also reports the number of 3D models, values for
the fixed and varying parameters, and the total number of
rendered images with 1024×1024 resolution in each track.
Fig. 7 depicts some example renders from each test track.
More details can be found in the SM Sec. 6.4.2. Ablation Studies
We propose an extensive analysis of each component of our
pipeline by using soft intersection over union ( IoU), root
mean squared error ( rmse ), its scaled version ( s-rmse ) [48]
and zero-normalized cross-correlation ( zncc) [18] as the
evaluation metrics.
4.2.1. Prediction Types
We trained our diffusion model using 4 different prediction
types presented in Sec. 3.2.1, namely ε,v, sample predic-
tions and rectified flow for 150kiterations with S(θ, ϕ, s )
conditionings. We trained each model on 4 Nvidia H100-
80GB GPUs with a batch size of 2 for about 2 days. We
used AdamW optimizer [28] with a learning rate of 1e−5.
Due to the stochastic nature of the diffusion models, the
performance of the models varies depending on the initial
noise. Hence, we compute mean and standard deviation val-
ues for each metric averaged over 10seeds for each image.
Our objective is to reduce the number of sampling steps
as much as possible to satisfy the requirements of practical
applications. Therefore, we first analyze how the models
trained with different prediction types perform for differ-
ent sampling steps. To this end, using all the images across
the three tracks, we compare the models in terms of IoU
values for 1,2,4,8,20sampling steps (more quantitative
results can be found in the SM Sec. 7). The first plot in
Fig. 8 demonstrates that rectified flow with only 1 step out-
performs the other prediction types with 20 inference steps.
It also shows that the performance gap between 1 and 20
steps for rectified flow is significantly smaller than for the
others. We report the quantitative results for 1 and 20 steps
using all the metrics on each individual track in Table. 2.
Secondly, we examine the behavior of each prediction
type across varying numbers of training iterations. Our mo-
tivation is to determine whether certain types of predictions
converge faster than others. For this purpose, we set the
number of sampling steps to 20, and we compute IoU val-
ues for 10k,20k,. . .,150ktraining iterations. As expected,
training longer improves the performance of the models, as
shown in Fig. 8. Interestingly, rectified flow again outper-
forms others with fewer training iterations. Its performance
with only 10kiterations is on par with the performance of ε

Pred Type Track 1 (Softness Control) Track 2 (Horz. Direc. Control) Track 3 (Vert. Direc. Control)
IoU↑ rmse↓ s-rmse↓ zncc↑ IoU↑ rmse↓ s-rmse↓ zncc↑ IoU↑ rmse↓ s-rmse↓ zncc↑
εpred. 0.048 0.421 0.106 0.021 0.056 0.425 0.118 0.018 0.045 0.426 0.109 0.016
sample 0.098 0.141 0.106 0.049 0.094 0.151 0.118 0.041 0.087 0.144 0.108 0.040
v-pred. 0.047 0.424 0.107 0.020 0.055 0.427 0.118 0.018 0.044 0.428 0.109 0.016
Rect. Flow 0.768 0.020 0.019 0.982 0.732 0.027 0.026 0.968 0.736 0.026 0.025 0.971
εpred. 0.598 0.029 0.027 0.970 0.540 0.041 0.037 0.942 0.532 0.036 0.034 0.946
sample 0.745 0.030 0.028 0.956 0.705 0.039 0.037 0.929 0.687 0.039 0.036 0.928
v-pred. 0.722 0.022 0.021 0.978 0.685 0.029 0.027 0.958 0.678 0.027 0.026 0.964
Rect. Flow 0.818 0.021 0.020 0.979 0.780 0.030 0.028 0.958 0.776 0.028 0.027 0.960
Table 2. The performance of models trained for 150kiterations with different prediction types on each track in terms of IoU,rmse ,s-rmse ,
andzncc metrics. The first and the last 4 rows report the quantitative results for 1 and 20 sampling steps respectively.
Figure 9. Example renders and the blob light maps (top-right).
Blob location and size encode the light location ( x,y) and size s.
012 4 8 200.70.8
Number of sampling stepsIoUOurs
Blob
Figure 10. Comparison between our timestep and blob condition-
ings trained for 150kiterations and 1 sampling step.
prediction with 150kiterations.
4.2.2. Other Conditioning Mechanisms
We compare our conditioning mechanism with other alter-
natives. SSN [40] proposes to represent the light sources as
a mixture of Gaussian, whose amplitude and variance en-
code the softness sand intensity Iof each light source. We
adapt this idea to our setting to represent the light source as
a Gaussian blob in a1024×1024 gray-scale image.
We generate a 2D Gaussian blob with a fixed size, re-
size it by a factor of sto represent the softness through the
Gaussian size, and position it on a black image using the
cartesian coordinates of the light source ( x,y) computed
from its spherical coordinates as ( x=rsin(θ) cos( ϕ),
y=rsin(θ) sin(ϕ)). Fig. 9 illustrates example renders and
the corresponding blob light maps. To condition the de-
noiser on the light map, we resize it to match the latent size
and concatenate it with the noisy latent and the V AE embed-
dings of the object image and the resized mask, increasing
the number of channels in the latent space to 2c+ 2. How-
ever, our conditioning approach presented in Sec. 3.2.3 is
simpler as it directly injects scalars to the denoiser rather
Figure 11. Intensity Control. The first two and the last two images
show results for the models with S(θ, ϕ, s )andS(θ, ϕ, s, I )con-
ditionings, respectively. We multiply the shadow map predicted
by the first model by Ito control the shadow intensity. I= 1 in
the 1st and the 3rd images, I= 0.5in the 2nd and the 4th images.
than using an external and complex light representation. It
also does not require adding an extra channel in the latent
space. However, interestingly, its performance is roughly
on par with the model utilizing an external blob representa-
tion for the same number of training iterations with rectified
flow and 1 sampling step, as demonstrated in Fig. 10.
4.3. Intensity Conditioning
While shadow intensity can be easily adjusted by multiply-
ing the predicted shadow with a fixed intensity by a scalar
I, we explore whether the intensity can be controlled in
the same way as the other light parameters S(θ, ϕ, s )with
our conditioning mechanism. When training the model
withS(θ, ϕ, s, I )conditioning, we multiply the shadow
maps from the training set by a random Ivalue, uniformly
sampled between 0.1and1.9, in each iteration and use
these modified maps as the ground truth. The output from
the model conditioned on S(θ, ϕ, s )and scaled by Iis
nearly identical to the output from the model conditioned
onS(θ, ϕ, s, I )as shown in Fig. 11. This ablation confirms
that our framework is flexible enough to easily incorporate
additional shadow controls.
4.4. Qualitative Analysis on Real Images
Given that our model is trained with fully synthetic data, the
goal of this section is to assess whether it generalizes well
to real images. To this end, we collected a set of foreground
object images, as well as target backgrounds. We conduct a
qualitative analysis for softness control, horizontal and ver-
tical shadow direction control. We also seek to understand

Target Bg. Image Object image Result 1 Result 2 Result 3Softness Control
θ= 30◦,ϕ= 60◦
s= 2 s= 5 s= 8Horiz. Dir. Control
θ= 30◦,s= 2
ϕ= 45◦ϕ= 135◦ϕ= 315◦Vert. Dir. Control
ϕ= 0◦,s= 2
θ= 0◦θ= 20◦θ= 40◦
Table 3. Shadow prediction on real images by using our method when controlling the softness sand position (θ, ϕ)of the light source.
whether the predicted shadows are visually appealing, and
their geometry aligns with the object as for synthetic data.
We run our full pipeline, involving the model trained for
150kiterations, with rectified flow using only 1 inference
step. For the softness control, we set the angles θandϕto
30◦and60◦(see Fig. 4), respectively, while varying the pa-
rameter samong the values 2,5, and 8. As illustrated in the
top row of Table 3, the predicted shadow softens progres-
sively as the value of sincreases.
For the horizontal shadow direction control, we set θto
30◦andsto2. We then move the light source horizon-
tally along the surface of the sphere at angles of ϕ= 45◦,
ϕ= 135◦, and ϕ= 315◦. The middle row illustrates that
the predicted shadow’s direction corresponds with the vary-
ing degrees of ϕ. For the analysis of vertical shadow direc-
tion control, we enforce the model to predict the shadow al-
ways on the left with a fixed softness by setting ϕ= 0◦and
s= 2, and predict shadows by positioning the light source
directly above the object at an angle of θ= 0◦, then move
it along the sphere to the right at θ= 20◦andθ= 40◦. As
illustrated at the bottom row in Table 3, the shadow initially
falls directly beneath the object and extends to the left as
we increase the value of θand demonstrates as well that the
predicted shadow aligns geometrically with the object.
These observations indicate that our model, trained en-tirely on our synthetic datasets, demonstrates strong gen-
eralization capabilities when tested with real images. We
provide more qualitative results in SM Sec. 8.
5. Conclusion
We presented a novel method for fast, controllable, and
background-free shadow generation for object images. To
achieve this, we created a large synthetic dataset using a
3D rendering engine, generating diverse shadow maps with
varying light parameters. We then trained a diffusion model
with a rectified flow objective enabling fast and high-quality
shadow generation while providing control over shadow di-
rection, softness, and intensity. Furthermore, we demon-
strated that our model generalizes well to real-world im-
ages, producing realistic and controllable shadows without
introducing any artifacts or color shifts on the background.
To enable further research in this area, we also released a
public benchmark dataset containing a diverse set of object
images and their corresponding shadow maps under various
settings. By harnessing the power of synthetic data and dif-
fusion models, we opened up new possibilities for creative
content creation.

References
[1] Clement Chadebec, Onur Tasar, Eyal Benaroche, and
Benjamin Aubin. Flash diffusion: Accelerating any
conditional diffusion model for few steps image gen-
eration, 2024. 2
[2] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor
Ionescu, and Mubarak Shah. Diffusion models in vi-
sion: A survey. PAMI , 45(9):10850–10869, 2023. 4
[3] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen
Yan, Yin Zhou, Leonidas Guibas, Dragomir
Anguelov, et al. Nerdi: Single-view nerf syn-
thesis with language-guided diffusion as general
image priors. In CVPR , pages 20637–20647, 2023. 2
[4] Maximilian Denninger, Martin Sundermeyer, Do-
minik Winkelbauer, Youssef Zidan, Dmitry Olefir,
Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan
Katam. Blenderproc, 2019. 2
[5] Patrick Esser, Sumith Kulal, Andreas Blattmann,
Rahim Entezari, Jonas M ¨uller, Harry Saini, Yam Levi,
Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.
Scaling rectified flow transformers for high-resolution
image synthesis. In ICML , 2024. 2
[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial
nets. NIPS , 27, 2014. 2
[7] Ezra Thess Mendoza Guevarra. Modeling and anima-
tion using blender: blender 2.80: the rise of Eevee .
Apress, 2019. 5
[8] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng
Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, and
Ying-Cong Chen. Lotus: Diffusion-based visual foun-
dation model for high-quality dense prediction, 2024.
2
[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 2
[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models. NIPS , 33:6840–6851,
2020. 2, 4, 5
[11] Yan Hong, Li Niu, and Jianfu Zhang. Shadow gen-
eration for composite image in real-world scenes. In
AAAI , pages 914–922, 2022. 2
[12] Yihan Hu, Yiheng Lin, Wei Wang, Yao Zhao, Yunchao
Wei, and Humphrey Shi. Diffusion for natural image
matting, 2023. 2
[13] Bernardo Iraci. Blender cycles: lighting and rendering
cookbook . Packt Publishing Ltd, 2013. 5
[14] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando
Metzger, Rodrigo Caye Daudt, and Konrad Schindler.
Repurposing diffusion-based image generators for
monocular depth estimation. In CVPR , 2024. 2[15] Bernhard Kerbl, Georgios Kopanas, Thomas
Leimk ¨uhler, and George Drettakis. 3d gaussian
splatting for real-time radiance field rendering. TOG ,
42(4), 2023. 2
[16] Diederik Kingma, Tim Salimans, Ben Poole, and
Jonathan Ho. Variational diffusion models. NIPS , 34:
21696–21707, 2021. 4
[17] Diederik P Kingma. Auto-encoding variational bayes,
2013. 4
[18] John P Lewis et al. Fast template matching. In Vision
interface , pages 15–19. Quebec City, QC, Canada,
1995. 6
[19] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sa-
bet, Linmiao Xu, and Suhail Doshi. Playground v2.5:
Three insights towards enhancing aesthetic quality in
text-to-image generation, 2024. 2
[20] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu,
Zhaoning Wang, Xuefeng Xiao, and Chen Chen.
Controlnet ++: Improving conditional controls with
efficient consistency feedback. In ECCV , pages 129–
147. Springer, 2025. 2
[21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu,
Maximilian Nickel, and Matthew Le. Flow matching
for generative modeling. In ICLR , 2023. 2, 4
[22] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
Mukund Varma T, Zexiang Xu, and Hao Su. One-
2-3-45: Any single image to 3d mesh in 45 seconds
without per-shape optimization. NIPS , 36, 2024. 2
[23] Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao,
Bo Zhang, and Li Niu. Shadow generation for com-
posite image using diffusion model. In CVPR , pages
8121–8130, 2024. 2
[24] Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis
Park, Simon Stent, and Carl V ondrick. Shadows shed
light on 3d objects, 2022. 2
[25] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel
Tokmakov, Sergey Zakharov, and Carl V ondrick.
Zero-1-to-3: Zero-shot one image to 3d object. In
CVPR , pages 9298–9309, 2023. 2
[26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow
straight and fast: Learning to generate and transfer
data with rectified flow. In ICLR , 2023. 4
[27] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng,
et al. Instaflow: One step is enough for high-quality
diffusion-based text-to-image generation. In ICLR ,
2023. 2
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight
decay regularization. In ICLR , 2019. 6
[29] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and
Hang Zhao. Latent consistency models: Synthesizing
high-resolution images with few-step inference, 2023.
2

[30] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik
Kingma, Stefano Ermon, Jonathan Ho, and Tim Sali-
mans. On distillation of guided diffusion models. In
CVPR , pages 14297–14306, 2023. 2
[31] Gal Metzer, Elad Richardson, Or Patashnik, Raja
Giryes, and Daniel Cohen-Or. Latent-nerf for shape-
guided generation of 3d shapes and textures. In CVPR ,
pages 12663–12673, 2023. 2
[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
Nerf: Representing scenes as neural radiance fields for
view synthesis. In ECCV , 2020. 2
[33] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu,
Jian Zhang, Zhongang Qi, and Ying Shan. T2i-
adapter: Learning adapters to dig out more control-
lable ability for text-to-image diffusion models. In
AAAI , pages 4296–4304, 2024. 2
[34] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna,
and Robin Rombach. Sdxl: Improving latent diffusion
models for high-resolution image synthesis, 2023. 2,
4, 5
[35] Robin Rombach, Andreas Blattmann, Dominik
Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion mod-
els. In CVPR , pages 10684–10695, 2022. 2, 4
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation. In MICCAI , pages 234–241. Springer,
2015. 2
[37] Tim Salimans and Jonathan Ho. Progressive distilla-
tion for fast sampling of diffusion models. In ICLR ,
2022. 4
[38] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and
Robin Rombach. Adversarial diffusion distillation. In
ECCV , pages 87–103. Springer, 2025. 2
[39] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi,
and David J. Fleet. Monocular depth estimation using
diffusion models, 2023. 2
[40] Yichen Sheng, Jianming Zhang, and Bedrich Benes.
Ssn: Soft shadow network for image compositing. In
CVPR , pages 4380–4390, 2021. 2, 5, 7
[41] Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin,
A Cengiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman,
and Bedrich Benes. Controllable shadow generation
using pixel height maps. In ECCV , pages 240–256.
Springer, 2022. 2
[42] Yichen Sheng, Jianming Zhang, Julien Philip, Yan-
nick Hold-Geoffroy, Xin Sun, He Zhang, Lu Ling, and
Bedrich Benes. Pixht-lab: Pixel height based light
effect generation for image compositing. In CVPR ,
pages 16643–16653, 2023. 2[43] Peter Shirley and R Keith Morley. Realistic ray trac-
ing. AK Peters, Ltd., 2008. 1, 5
[44] Jascha Sohl-Dickstein, Eric Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In
ICML , pages 2256–2265. PMLR, 2015. 2, 4
[45] Yang Song, Jascha Sohl-Dickstein, Diederik P
Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through
stochastic differential equations. In ICLR , 2020. 2,
4
[46] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen,
Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel
Aliaga. Objectstitch: Object compositing with diffu-
sion model. In CVPR , pages 18310–18319, 2023. 2
[47] David E Stewart. Rigid-body dynamics with friction
and impact. SIAM review , 42(1):3–39, 2000. 3
[48] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zex-
iang Xu, Xueming Yu, Graham Fyffe, Christoph Rhe-
mann, Jay Busch, Paul Debevec, and Ravi Ramamoor-
thi. Single image portrait relighting. TOG , 38(4):1–12,
2019. 6
[49] J Dennis Thomas. The art and style of product pho-
tography . John Wiley & Sons, 2013. 1
[50] Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu,
Jinwei Gu, Yung-Yu Chuang, and Shin’ichi Satoh.
Matting by generation. In SIGGRAPH , 2024. 2
[51] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael
Pritch, Alex Rav-Acha, and Yedid Hoshen. Object-
drop: Bootstrapping counterfactuals for photorealistic
object removal and insertion, 2024. 2
[52] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi
Wang, and Zhangyang Wang. Neurallift-360: Lifting
an in-the-wild 2d photo to a 3d object with 360deg
views. In CVPR , pages 4479–4489, 2023. 2
[53] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo
Hou. Ufogen: You forward once large scale text-to-
image generation via diffusion gans. arXiv preprint
arXiv:2311.09257 , 2023. 2
[54] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong,
Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and
Ming-Hsuan Yang. Diffusion models: A comprehen-
sive survey of methods and applications. ACM Com-
puting Surveys , 56(4):1–39, 2023. 4
[55] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo,
Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu,
and Xiaoguang Han. Stablenormal: Reducing diffu-
sion variance for stable and sharp normal. TOG , 2024.
2
[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
Adding conditional control to text-to-image diffusion
models. In CVPR , pages 3836–3847, 2023. 2

[57] Shuyang Zhang, Runze Liang, and Miao Wang. Shad-
owgan: Shadow synthesis for virtual objects with con-
ditional adversarial networks. CVM , 5:105–115, 2019.
2

Controllable Shadow Generation with
Single-Step Diffusion Models from Synthetic Data
Supplementary Material
Figure 12. Number of 3D meshes for each category in our synthetic dataset.
6. Synthetic Dataset
As mentioned in Sec. 4.1, we gathered 9,922 3D
meshes created by professional artists publicly available on
blenderkit2with a free of use, representing a diverse array
of real-world object categories. Fig. 12 displays the cate-
gory names and number of 3D meshes for each category.
Our public benchmark comprises 3 tracks: softness
control, horizontal shadow direction control, and vertical
shadow direction control. Figs. 13, 14, 15 respectively il-
lustrate some rendered images from our tracks for horizon-
tal shadow control, vertical shadow control, and softness
control.
7. Quantitative Analysis
Fig. 8 compares models trained with various prediction
types across multiple sampling steps and training iterations,
using only IoU as the evaluation metric. The figure presents
the average values computed from all images over all our
3 tracks across 10 seeds as a curve, accompanied by a
semi-transparent margin indicating the standard deviation.
We display the same plots for IoU, RMSE, S-RMSE, and
ZNCC metrics in Figs. 16 and 17.
Fig. 16 supports our conclusion we draw in Sec. 4.2.1
based on Fig. 8 by demonstrating that rectified flow signifi-
cantly outperforms the other methods for one sampling step
across various metrics. In contrast, the other techniques re-
quire more sampling steps to achieve comparable perfor-
mance. Similary, Fig. 17 demonstrates that rectified flow
attains higher performance with fewer training iterations.
2https://www.blenderkit.comBoth figures reveal that the standard deviation of the results
for the rectified method is significantly smaller. This means
the predicted shadow exhibits less variation for a given seed,
making rectified flow more robust to changes in the seed.
8. Qualitative Analysis on Real Images
For a comprehensive qualitative analysis, we gathered a di-
verse collection of object images from Unsplash3and Pex-
els4. Some 3D meshes, such as those in the seating set
category (see Fig. 12), consist of multiple objects. Conse-
quently, we assembled a test set that includes both single-
object images and those containing multiple objects. We
applied our model trained on our synthetic dataset for 150k
iterations with S(θ, ϕ, s )conditionings to those real images.
To qualitatively evaluate the performance of our model
for softness control, we hold θandϕconstant while vary-
ings, as shown in Figs. 18 and 19. To control the horizontal
shadow direction, we vary ϕwhile keeping θandscon-
stant. Similarly, for vertical direction control, we adjust θ
and hold the other two light parameters fixed. Figs. 20 and
21 illustrate the visual outcomes for horizontal and vertical
control, respectively. Finally, Fig. 22 shows the predicted
shadows when all light parameters are adjusted.
Figures 18, 19, 20, 21, and 22 illustrate that our model,
trained solely on a fully synthetic dataset, accurately pre-
dicts high-quality shadows in real images containing both
single and multiple objects. In addition to this, the pre-
dicted shadow’s direction and softness align precisely with
the specified θ,ϕ, and svalues.
3https://unsplash.com
4https://pexels.com

(a)ϕ= 0◦
(b)ϕ= 20◦
(c)ϕ= 40◦
(d)ϕ= 60◦
(e)ϕ= 80◦
(f)ϕ= 100◦
(g)ϕ= 120◦
(h)ϕ= 140◦
(i)ϕ= 160◦
(j)ϕ= 180◦
(k)ϕ= 200◦
(l)ϕ= 220◦
(m)ϕ= 240◦
(n)ϕ= 260◦
(o)ϕ= 280◦
(p)ϕ= 300◦
(q)ϕ= 320◦
(r)ϕ= 340◦
Figure 13. Renders for one 3D mesh from the horizontal shadow direction control track. θ= 30◦ands= 2.
(a)θ= 10◦
(b)θ= 15◦
(c)θ= 20◦
(d)θ= 25◦
(e)θ= 30◦
(f)θ= 35◦
(g)θ= 40◦
(h)θ= 45◦
(i)θ= 45◦
(j)θ= 5◦
(k)θ= 10◦
(l)θ= 15◦
(m)θ= 20◦
(n)θ= 25◦
(o)θ= 30◦
(p)θ= 35◦
(q)θ= 40◦
(r)θ= 45◦
Figure 14. Renders for two 3D meshes from the vertical shadow direction control track. ϕ= 0◦ands= 2.

(a)s= 2
 (b)s= 4
 (c)s= 8
 (d)s= 2
 (e)s= 4
 (f)s= 8
Figure 15. Renders for two 3D meshes from the softness control track. θ= 30◦andϕ= 0◦.
012 4 8 2000.20.40.60.81IoUε-pred. rectified flow sample pred. v-pred.
012 4 8 2000.20.40.6RMSE
012 4 8 2000.10.2S-RMSE
012 4 8 2000.20.40.60.81
Number of sampling stepsZNCC
Figure 16. Plots comparing methods trained for 150kiterations,
across 4 prediction types and 4 metrics, for various sampling steps.
Half transparent margin represents the standard deviation.
010k 30k 50k 70k 90k 110k 130k 150k00.20.40.60.81IoUε-pred. rectified flow sample pred. v-pred.
010k 30k 50k 70k 90k 110k 130k 150k00.10.2RMSE
010k 30k 50k 70k 90k 110k 130k 150k00.10.2S-RMSE
010k 30k 50k 70k 90k 110k 130k 150k00.20.40.60.81
Number of training iterationsZNCCFigure 17. Plots comparing methods trained for various iterations,
across 4 prediction types and 4 metrics, for 20sampling steps.
Half transparent margin represents the standard deviation.

(a) Input Image
 (b)s= 2
 (c)s= 5
 (d)s= 8
(e) Input Image
 (f)s= 2
 (g)s= 5
 (h)s= 8
Figure 18. Softness control. The fixed light parameters are θ= 30◦andϕ= 150◦for the top row, θ= 30◦andϕ= 50◦for the bottom
row. The varying parameter in both cases is sto change the softness.
(a) Input image
 (b)s= 2
 (c)s= 4
 (d)s= 6
(e) Input image
 (f)s= 3
 (g)s= 5
 (h)s= 7
Figure 19. Softness control. The fixed light parameters are θ= 20◦andϕ= 90◦for the top row, θ= 35◦andϕ= 135◦for the bottom
row. The varying parameter in both cases is sto change the softness.

(a) Input image
 (b)ϕ= 135◦
(c)ϕ= 180◦
(d)ϕ= 225◦
(e) Input image
 (f)ϕ= 50◦
(g)ϕ= 90◦
(h)ϕ= 120◦
Figure 20. Horizontal shadow direction control. The fixed light parameters are θ= 30◦ands= 2for the top row, θ= 40◦ands= 3for
the bottom row. The varying parameter in both cases is ϕto move the shadow horizontally.
(a) Input image
 (b)θ= 0◦
(c)θ= 25◦
(d)θ= 45◦
(e) Input image
 (f)θ= 5◦
(g)θ= 25◦
(h)θ= 40◦
Figure 21. Vertical shadow direction control. The fixed light parameters are ϕ= 315 ands= 2 for the top row, ϕ= 160 ands= 4 for
the bottom row. The varying parameter in both cases is θto move the shadow vertically.

(a) Input Image
 (b)θ= 20◦, ϕ= 180◦, s= 2
 (c)θ= 40◦, ϕ= 310◦, s= 2
 (d)θ= 20◦, ϕ= 50◦, s= 4
(e) Input Image
 (f)θ= 30◦, ϕ= 340◦, s= 5
 (g)θ= 40◦, ϕ= 110◦, s= 6
 (h)θ= 40◦, ϕ= 180◦, s= 4
(i) Input Image
 (j)θ= 20◦, ϕ= 60◦, s= 4
 (k)θ= 35◦, ϕ= 130◦, s= 6
 (l)θ= 35◦, ϕ= 225◦, s= 2
(m) Input Image
 (n)θ= 26◦, ϕ= 34◦, s= 1
 (o)θ= 38◦, ϕ= 196◦, s= 7
 (p)θ= 40◦, ϕ= 180◦, s= 4
Figure 22. Shadow direction and softness control by changing the value for each of θ,ϕands.

